{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c947842",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- find_dotenv() searches for a .env file in your current directory and parent directories.\n",
    "- load_dotenv() reads the .env file and loads any variables (like API keys) into your environment, so you can access them with os.environ[\"VARIABLE_NAME\"].\n",
    "-  Setting verbose=True enables detailed logging of the internal operations and steps taken by the chain during execution. This means that when you run the conversation, you will see additional output in your console or notebook, such as the prompts sent to the language model, the responses received, and any intermediate processing steps.\n",
    "- Using verbose=True is especially helpful for debugging and understanding how the chain processes inputs and generates outputs. It allows you to trace the flow of data and see exactly what is happening behind the scenes, which can be valuable when developing or troubleshooting conversational AI applications. If you prefer a cleaner output without these details, you can set verbose=False or simply omit the parameter, as it defaults to False.\n",
    "- You can see conversastion history with verbose=True\n",
    "- ConversationBufferMemory is how u store memory but u can also add to this memory\n",
    "- llms are stateless ie. dont have memory by default. the conversation is typically passed as context to use when responding. you have to think about how expensive this can be as the chat grows, so u may want to choose other methods provided by langchain like ConversationBufferWindowMemory \n",
    "- ConversationBufferWindowMemory only stores a window of memory that u can specify\n",
    "- ConversationTokenBufferMemory stores certain amm tokens. since most llm pricing is about tokens, this maps more directly to cost\n",
    "- ConversationSummaryMemory summarises convo & makes that the memory\n",
    "- also vector data memory which stores text in vector db & retrieves mosst relevant block of text\n",
    "- Entity memory remembers details about specific entities\n",
    "- you can combine diff types of memory eg entity + conversaion\n",
    "- can store in normal db either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2cae546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) #read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d39fa07",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eb1d30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "89a93e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "567fc258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hi my names nicole\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Nicole! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"hi my names nicole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a4697a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hi my names nicole\n",
      "AI: Hello Nicole! It's nice to meet you. How can I assist you today?\n",
      "Human: im 25\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's great to know, Nicole! Being 25 is a wonderful age with so many opportunities ahead. Is there anything specific you'd like to know or talk about today?\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"im 25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b5565953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: hi my names nicole\n",
      "AI: Hello Nicole! It's nice to meet you. How can I assist you today?\n",
      "Human: im 25\n",
      "AI: That's great to know, Nicole! Being 25 is a wonderful age with so many opportunities ahead. Is there anything specific you'd like to know or talk about today?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "67101bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: hi my names nicole\\nAI: Hello Nicole! It's nice to meet you. How can I assist you today?\\nHuman: im 25\\nAI: That's great to know, Nicole! Being 25 is a wonderful age with so many opportunities ahead. Is there anything specific you'd like to know or talk about today?\"}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#access everything langchain has stored\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "285fc267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: hi my names nicole\n",
      "AI: Hello Nicole! It's nice to meet you. How can I assist you today?\n",
      "Human: im 25\n",
      "AI: That's great to know, Nicole! Being 25 is a wonderful age with so many opportunities ahead. Is there anything specific you'd like to know or talk about today?\n",
      "Human: Hi\n",
      "AI: hi, how are you!\n"
     ]
    }
   ],
   "source": [
    "# add to memory (appends to end)\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hi\"}, {\"output\": \"hi, how are you!\"}\n",
    ")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c5cf4",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7b3d977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e92371a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k: Use to keep track of the last k turns of a conversation.\n",
    "# If the number of messages in the conversation is more than the maximum number of messages to keep, the oldest messages are dropped.\n",
    "# k=1 means 1 convo exchang ie 1 question & its ans\n",
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "53370228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: whats the weather\n",
      "AI: theres currently a heat wave\n"
     ]
    }
   ],
   "source": [
    "# add to memory (appends to end)\n",
    "# notice how it only keeps the last\n",
    "memory.save_context(\n",
    "    {\"input\": \"Hi\"}, {\"output\": \"hi, how are you!\"}\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"whats the weather\"}, {\"output\": \"theres currently a heat wave\"}\n",
    ")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bfbec7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory=ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7d0151b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hi my name is nicole\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Nicole! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"hi my name is nicole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d51deedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: hi my name is nicole\n",
      "AI: Hello Nicole! It's nice to meet you. How can I assist you today?\n",
      "Human: whats 1+1\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"whats 1+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "74a76ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: whats 1+1\n",
      "AI: 1 + 1 equals 2. Is there anything else you would like to know?\n",
      "Human: whats my name\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to personal information such as your name. Is there anything else you would like to know?\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wont remember this since k=1\n",
    "conversation.predict(input=\"whats my name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb1307",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fd22bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f8a9011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm= ChatOpenAI(temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3516fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=10)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c6f46322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "memory.load_memory_variables({})\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3ee87963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "memory.max_token_limit=2\n",
    "memory.load_memory_variables({})\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27afedeb",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ca33d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2ffe1532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory2 = ConversationBufferMemory(llm=llm, max_token_limit=5)\n",
    "memory2.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory2.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory2.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1f1d2a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory2.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fbc60340",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory2,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d074211e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello\n",
      "AI: What's up\n",
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: What is on the schedule today?\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I would recommend showcasing the latest Language Model demo, as it is a cutting-edge technology that your customer will likely be very interested in. It demonstrates the power and capabilities of our AI technology in a tangible way. Additionally, you could also show some real-world examples of how our AI has helped other customers improve their business processes and outcomes.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d08848ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\\nHuman: What would be a good demo to show?\\nAI: I would recommend showcasing the latest Language Model demo, as it is a cutting-edge technology that your customer will likely be very interested in. It demonstrates the power and capabilities of our AI technology in a tangible way. Additionally, you could also show some real-world examples of how our AI has helped other customers improve their business processes and outcomes.\"}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory2.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
